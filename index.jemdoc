
~~~
{}{img_left}{./logos/hands.png}{HANDS}{100%}{}{https://hands-workshop.org/}
~~~

{{
<div id="beamer">
<beam> 
Observing and Understanding <b>Hands</b> in Action
</beam></br>
<beams>
in conjunction with ECCV 2024</br>
</beams>
</div>
}}

= Overview 

~~~
{}{raw}
<font size="5">  
    Welcome to our HANDS@ECCV24.
</font>  
</br>
</br>
~~~


Our HANDS workshop will gather vision researchers working on perceiving hands performing actions, including 2D & 3D hand detection, segmentation, pose/shape estimation, tracking, etc. We will also cover related applications including gesture recognition, hand-object manipulation analysis, hand activity understanding, and interactive interfaces.

The eighth edition of this workshop will emphasize the use of large foundation models (/e.g./ CLIP, Point-E, Segment Anything, Latent Diffusion Models) for hand-related tasks. These models have revolutionized the perceptions of AI, and demonstrate groundbreaking contributions to multimodal understanding, zero-shot learning, and transfer learning. However, there remains an untapped potential for exploring their applications in hand-related tasks.


~~~
{}{raw}
<style>
  .left-column {
    float: left;
    width: 50%;
    line-height: 1.5;
    height: 280px;
  }

  .right-column {
    float: right;
    width: 50%;
    line-height: 1.5;
    height: 280px;
  }
</style>

<div class="left-column">
<ul>
<li>Hand pose and shape estimation </li>
<li>Hand & object interactions</li>
<li>Hand detection/segmentation</li>
<li>Hand gesture/action recognition</li>
<li>4D hand tracking and motion capture</li>
<li>Hand motion synthesis</li>
<li>Generalization and adaptation</li>
<li>Robot grasping, object manipulation</li>
</ul>
</div>

<div class="right-column">
<ul>
<li>Hand modeling, rendering, generation</li>
<li>Camera systems and annotation tools</li>
<li>Novel algorithms and network architectures</li>
<li>Multi-modal learning</li>
<li>Self-/un-/weakly-supervised learning</li>
<li>Egocentric vision for AR/VR</li>
<li>Haptics</li>
</ul>
</div>

~~~

= Schedule
TBD

= Call for Papers
We will call for full length submissions with published proceedings via the CMT system. Also, we will invite submission of 2-3 page extended abstracts and recent hand-related papers/posters to the workshop.

= Challenges
We will present the HANDS24 challenge based on [https://assemblyhands.github.io/ AssemblyHands], [https://arctic.is.tue.mpg.de/ ARCTIC], [https://oakink.net/v2/ OakInk2] and [https://research.facebook.com/publications/umetrack-unified-multi-view-end-to-end-hand-tracking-for-vr/ UmeTrack]. More details will be released later. Looking forward to your participation.

= Invited Speakers
TBD

= Organizers
{{
<div id="member-container">
    <div id="member">
        <img src="./profiles/hyung.jpg">
        <p>
            <b><a href="https://hyungjinchang.wordpress.com">Hyung Jin Chang</a></b></br>
            University of Birmingham
        </p>
    </div>
    <div id="member">
        <img src="./profiles/zicong.png">
        <p>
            <b><a href="https://zc-alexfan.github.io">Zicong Fan</a></b></br>
            ETH Zurich
        </p>
    </div> 
    <div id="member">
        <img src="./profiles/otmar.png">
        <p>
            <b><a href="https://ait.ethz.ch/people/hilliges">Otmar Hilliges</a></b></br>
            ETH Zurich
        </p>
    </div>
    <div id="member">
        <img src="./profiles/kun.png">
        <p>
            <b><a href="https://kunhe.github.io">Kun He</a></b></br>
            Meta Reality Labs
        </p>
    </div>
    <div id="member">
        <img src="./profiles/take.png">
        <p>
            <b><a href="https://tkhkaeio.github.io/">Take Ohkawa</a></b></br>
            University of Tokyo
        </p>
    </div>
        <div id="member">
        <img src="./profiles/yoichi.png">
        <p>
            <b><a href="https://sites.google.com/ut-vision.org/ysato/home">Yoichi Sato</a></b></br>
            University of Tokyo
        </p>
    </div>
         <div id="member">
        <img src="./profiles/linlin.png">
        <p>
            <b><a href="https://mu4yang.com">Linlin Yang</a></b></br>
            Communication University of China 
        </p>
    </div>
           <div id="member">
        <img src="./profiles/lixin.png">
        <p>
            <b><a href="https://lixiny.github.io">Lixin Yang</a></b></br>
            Shanghai Jiao Tong University
        </p>
    </div>
    <div id="member">
        <img src="./profiles/angela.png">
        <p>
            <b><a href="https://www.comp.nus.edu.sg/cs/people/ayao/">Angela Yao</a></b></br>
            National University of Singapore
        </p>
    </div>
     <div id="member">
        <img src="./profiles/linguang.png">
        <p>
            <b><a href="https://lg-zhang.github.io/">Linguang Zhang</a></b></br>
            Facebook Reality Labs (Oculus)
        </p>
    </div>
</div>
}}

= Contact
lyang@cuc.edu.cn
