<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta name="keywords"
    content="hands,ECCV 2024,workshop,pose estimation,MANO,challenge,keypoint,gesture,robot,grasping,manipulation,hand tracking,motion capture">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />


  <link rel="stylesheet" href="main.css" type="text/css" />
  <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
  <!--- <title></title> --->
  <title>HANDS Workshop</title>
  <!-- MathJax -->

  <!-- End MathJax -->
</head>

<body>
  <div id="main-container">
    <div id="header-container">
      <div id="header">
        <div id="header-icon-text-container">
          <div id="header-text-container">
            <nav class="style1">
              <ul id="outer_list">
                <li id="outer_li_year"><a id="current_year" href="#">2024<span id="arrow"></span></a>
                  <ul id="top_list">
                    <!-- <li id="style2"><a id="style3" href="workshop2023.html">2023</a></li> -->
                    <li id="style2"><a id="style3" href="workshop2024.html">2024</a></li>
                  </ul>
                </li>

                <li id="outer_li"><a id="workshop_link" href="#">Workshop</a>
                </li>

                <li id="outer_li"><a id="challenge_link" href="#">Challenge</a>
                </li>

              </ul>
            </nav>
          </div>
        </div>

      </div>
      <div id="layout-content">
        <div id="text-img-container">
          <div id="img-container">
            <a href="https://hands-workshop.org/"><img src="./logos/hands.png" alt="HANDS" width="100%" /></a>
          </div>
          <div id="text-container"></div>
        </div>
        <p>
        <div id="beamer">
          <beam>
            Observing and Understanding <b>Hands</b> in Action
          </beam></br>
          <beams>
            in conjunction with ECCV 2024</br>
          </beams>
        </div>

        <br>
        <div id="menu-container">
          <div id="menu-item"><a id="style6" href="#overview">Overview</a></div>
          <div id="menu-item"><a id="style6" href="#schedule">Schedule</a></div>
          <div id="menu-item"><a id="style6" href="#papers">Papers</a></div>
          <div id="menu-item"><a id="style6" href="#speakers">Speakers</a></div>
          <div id="menu-item"><a id="style6" href="#organizers">Organizers</a></div>
          <div id="menu-item"><a id="style6" href="#sponsors">Sponsors</a></div>
          <div id="menu-item"><a id="style6" href="#contact">Contact</a></div>
        </div>
        <br>
        </p>
        <h1 id="overview">Overview </h1>
        <font size="5">
          Welcome to our HANDS@ECCV24.
        </font>
        </br>
        </br>
        <p>Our HANDS workshop will gather vision researchers working on perceiving hands performing actions, including
          2D
          &amp; 3D hand detection, segmentation, pose/shape estimation, tracking, etc. We will also cover related
          applications including gesture recognition, hand-object manipulation analysis, hand activity understanding,
          and
          interactive interfaces.</p>
        <p>The eighth edition of this workshop will emphasize the use of large foundation models (<i>e.g.</i> CLIP,
          Point-E, Segment Anything, Latent Diffusion Models) for hand-related tasks. These models have revolutionized
          the
          perceptions of AI, and demonstrate groundbreaking contributions to multimodal understanding, zero-shot
          learning,
          and transfer learning. However, there remains an untapped potential for exploring their applications in
          hand-related tasks. </p>


        <h1 id="schedule">Tentative Schedule (Italy Time)</h1>
        <p style="align-items: center;text-align: center;"><b>September 30th (2 pm-6 pm), 2024</b></p></br>
        <p style="align-items: center;text-align: center;"><b>Room Suite 8, MiCo Milano</b></p></br>
        
        <table class="dataintable">
          <tbody>
            <tr>
              <td><b>14:00 - 14:10</b></td>
              <td>Opening Remarks</td>
            </tr>
            <tr>
              <td><b>14:10 - 14:40</b></td>
              <td> Invited Talk: Hanbyul Joo</td>
            </tr>
            <tr>
              <td><b>14:40 - 15:10</b></td>
              <td> Invited Talk: Shubham Tulsiani</td>
            </tr>
            <tr>
              <td><b>15:10 - 16:10</b></td>
              <td> Coffee break time & Poster </td>
            </tr>
            <tr>
              <td> <b>16:10 - 16:40</b></td>
              <td> Invited Talk: Qi Ye</td>
            </tr>
            <tr>
              <td> <b>16:40 - 17:10</b></td>
              <td> Invited Talk: Shunsuke Saito</td>
            </tr>
            <tr>
              <td><b>17:10 - 17:25</b></td>
              <td> Invited Talk: Prithviraj Banerjee </td>
            </tr>
             <tr>
              <td> <b>17:25 - 17:32</b></td>
              <td> Technical Report 1</td>
            </tr>
            <tr>
              <td> <b>17:32 - 17:39</b></td>
              <td> Technical Report 2</td>
            </tr>
            <tr>
              <td> <b>17:39 - 17:46</b></td>
              <td> Technical Report 3</td>
            </tr>
            <tr>
              <td> <b>17:46 - 17:53</b></td>
              <td> Technical Report 4</td>
            </tr>
            <tr>
              <td> <b>17:53 - 18:00</b></td>
              <td> Closing Remarks</td>
            </tr>
          </tbody>
        </table> 


        <h1 id="papers">Accepted Papers & Extended Abstracts</h1>

        <!-- list papers in the form of:
OakInk2 : A Dataset for Long-Horizon Hand-Object Interaction and Complex Manipulation Task Completion. 
Xinyu Zhan*, Lixin Yang*, Kangrui Mao, Hanlin Xu, Yifei Zhao, Zenan Lin, Kailin Li, Cewu Lu. 
[pdf] -->
        <p>We are delighted to announce the following accepted papers and extended abstracts will appear in the workshop! All full-length papers, extended abstracts and invited posters should prepare posters for communication during the workshop.</p> </br>   

        <p> <b>Poster size: the posters should be portrait (vertical), with a maximum size of 90x180 cm.</b></p>      

        <!-- <ul> -->
          <h2>Full-length Papers</h2>
          <ul>
            <li> AirLetters: An Open Video Dataset of Characters Drawn in the Air <br>
              <i>Rishit Dagli, Guillaume Berger, Joanna Materzynska, Ingo Bax, Roland Memisevic</i> <br>
              <!-- [pdf] -->
            </li>
            <a href="files/2024/airletters.pdf">[pdf]</a>
          </ul>
          <ul>
            <li> RegionGrasp: A Novel Task for Contact Region Controllable Hand Grasp Generation <br>
              <i>Yilin Wang, Chuan Guo, Li Cheng, Hai Jiang</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/regiongrasp.pdf">[pdf]</a>
            </li>
          </ul>
          <ul>
            <li> Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling <br>
              <i>Yilin Wen, Hao Pan, Takehiko Ohkawa, Lei Yang, Jia Pan, Yoichi Sato, Taku Komura, Wenping Wang</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/adaptive.pdf">[pdf]</a>
            </li>
          </ul>
          <ul>
            <li> Adaptive Multi-Modal Control of Digital Human Hand Synthesis using a Region-Aware Cycle Loss <br>
              <i>Qifan Fu, Xiaohang Yang, Muhammad Asad, Changjae Oh, Shanxin Yuan, Gregory Slabaugh</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/IHPT__ECCVW_2024_.pdf">[pdf]</a>
            </li>
          </ul>
          <ul>
            <li> Conditional Hand Image Generation using Latent Space Supervision in Random Variable Variational Autoencoders <br>
              <i>Vassilis Nicodemou, Iason Oikonomidis , Giorgos Karvounas, Antonis Argyros</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/ECCVW_Hands_2024_CG_SRV_VAE-4.pdf">[pdf]</a>
            </li>
          </ul>
          <ul>
            <li> ChildPlay-Hand: A Dataset of Hand Manipulations in the Wild <br>
              <i>Arya Farkhondeh, Samy Tafasca, Jean-Marc ODOBEZ</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/childplay-hand.pdf">[pdf]</a>
            </li>
          </ul>
          <ul>
            <li> EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos <br>
              <i>Masashi Hatano, Ryo Hachiuma, Hideo Saito</i> <br>
              <!-- [pdf] -->
              <a href="files/2024/emag.pdf">[pdf]</a>
            </li>
          </ul>
        <!-- </ul> -->

        <h2>Extended Abstracts</h2>
        <ul>
          <li> AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation <br>
            <i>Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Jose J Guerrero, Giovanni Maria Farinella, Antonino Furnari</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/affordances.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Diffusion-based Interacting Hand Pose Transfer <br>
            <i>Junho Park,
              Yeieun Hwang,
              Suk-Ju Kang</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/IHPT__ECCVW_2024_.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? <br>
            <i>Rosario Leonardi,
              Antonino Furnari,
              Francesco Ragusa,
              Giovanni Maria Farinella</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/[ABSTRACT] Are_Synthetic_Data_Useful_for_Egocentric_Hand_Object_Interaction_Detection.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer <br>
            <i>Xueyi Liu,
              Kangbo Lyu,
              jieqiong zhang,
              Tao Du,
              Li Yi</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/QuasiSim_short_abstract_2.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild <br>
            <i>Nie Lin,
              Takehiko Ohkawa,
              Mingfang Zhang,
              Yifei Huang,
              Ryosuke Furuta,
              Yoichi Sato</i> <br>
            <!-- add pdf from files/2024 -->
            <a href="files/2024/Nie_Lin_Extended_Abstracts_HANDS2024.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers <br>
            <i>An-Lun Liu, Yu-Wei Chao, Yi-Ting Chen</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/ECCV24 Extended Abstracts Submission - Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Action Scene Graphs for Long-Form Understanding of Egocentric Videos <br>
            <i>Ivan Rodin*, Antonino Furnari*, Kyle Min*, Subarna Tripathi, Giovanni Maria Farinella</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/EASG___HANDS.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos <br>
            <i>Zhifan Zhu, Dima Damen</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/HANDS_2024_Workshop__Get_a_Grip.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Self-Supervised Learning of Deviation in Latent Representation for Co-speech Gesture Video Generation <br>
            <i>Huan Yang, Jiahui Chen, Chaofan Ding, Runhua Shi, Siyu Xiong, Qingqi Hong, Xiaoqi Mo, Xinhan Di</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/ssl.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> OCC-MLLM-Alpha:Empowering Multi-modal Large Language Model for the Understanding of Occluded Objects with Self-Supervised Test-Time Learning <br>
            <i>Shuxin Yang, Xinhan Di</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/OCC_MLLM_Alpha_Empowering_Multimodal_Large_Language_Model_For_the_Understanding_of_Occluded_Objects.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera <br>
            <i>Zhengdi Yu, Alara Dirik, Stefanos Zafeiriou, Tolga Birdal</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/Dyn_HaMR_ECCVW_2024_extended_abstract.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> Learning Dexterous Object Manipulation with a Robotic Hand via
            Goal-Conditioned Visual Reinforcement Learning Using Limited Demonstrations <br>
            <i>Samyeul Noh, Hyun Myung</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/learning.pdf">[pdf]</a>
          </li>
        </ul>

        <h2>Invited Posters</h2>
        <ul>
          <li> AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild <br>
            <i>Junho Park, Kyeongbo Kong, Suk-Ju Kang</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/attentionhand.pdf">[pdf]</a>
          </li>
        </ul>
        <ul>
          <li> HandDAGT : A Denoising Adaptive Graph Transformer  for 3D Hand Pose Estimation <br>
            <i>Wencan Cheng, Eunji Kim, Jong Hwan Ko</i> <br>
            <!-- [pdf] -->
            <a href="files/2024/ECCV24_poster_HandDAGT.pdf">[poster]</a>
          </li>
        </ul>
        <ul>
          <li> On the Utility of 3D Hand Poses for Action Recognition <br>
            <i>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</i> <br>
            <!-- [pdf] -->
            <a href="https://arxiv.org/pdf/2403.09805">[pdf]</a>
          </li>
        </ul>

        <!-- <h2>Important Dates (Deadline has been extended)</h2>

        <table class="dataintable">
          <tbody>
            <tr>
              <td><b>June 15, 2024 (Opened)</b></td>
              <td>Paper Submission Start</td>
            </tr>
            <tr>
              <td><del>July 25</del> <b>August 10, 2024 11:59 (Pacific Time)</b></td>
              <td>Full Length Paper and its Supplementary Materials Submission Deadline</td>
            </tr>
            <tr>
              <td><b>August 15, 2024, 11:59 (Pacific Time)</b></td>
              <td>CMT for Extended Abstracts & Posters Submission is Closed, Further Submission can be submitted via Email (hands2024@googlegroups.com)</td>
            </tr>
            <tr>
              <td><del>August 10</del> <b>August 20, 2024, 11:59 (Pacific Time)</b></td>
              <td>Full Length Paper Author Notification, Extended Abstracts & Posters Submission Deadline</td>
            </tr>
            <tr>
              <td><del>August 20</del> <b>August 25, 2024, 11:59 (Pacific Time)</b></td>
              <td>Camera-ready Submission</td>
            </tr>
          </tbody>
        </table> -->

<!-- 
        <h2>Topics</h2>
        We will cover all hand-related topics. The relevant topics include and not limited to:

        <ul id="topicstyle1">
          <li id="topicstyle2">Hand pose and shape estimation</li>
          <li id="topicstyle2">Hand & object interactions</li>
          <li id="topicstyle2">Hand detection/segmentation</li>
          <li id="topicstyle2">Hand gesture/action recognition</li>
          <li id="topicstyle2">4D hand tracking and motion capture</li>
          <li id="topicstyle2">Hand motion synthesis</li>
          <li id="topicstyle2">Hand modeling, rendering, generation</li>
          <li id="topicstyle2">Camera systems and annotation tools</li>
          <li id="topicstyle2">Novel algorithms and network architectures</li>
          <li id="topicstyle2">Multi-modal learning</li>
          <li id="topicstyle2">Self-/un-/weakly-supervised learning</li>
          <li id="topicstyle2">Generalization and adaptation</li>
          <li id="topicstyle2">Egocentric vision for AR/VR</li>
          <li id="topicstyle2">Robot grasping, object manipulation, Haptics</li>
        </ul>
-->


        <!-- <h2>Submission Guidelines</h2>

        <b>Submission Website:</b> <a
          href="https://cmt3.research.microsoft.com/HANDS2024">https://cmt3.research.microsoft.com/HANDS2024</a>
        </br>
        </br>

        <p>We accept <b>full length papers</b>, <b>extended abstracts</b> and <b>posters</b> in our workshop. The full
          length submissions should be anonymized and will be one-round peer reviewed. The accepted full length papers
          will be included in the ECCV24 conference proceedings, while others will only be presented in the workshop. We
          welcome papers that are accepted to the ECCV2024 main conference or previous conferences, and extended
          abstracts that in progress, to show the posters in our workshop.</p>

        <h3>Call for full length papers</h3>

        <p>Full length submissions should follow <a
            href="https://eccv2024.ecva.net/Conferences/2024/AuthorGuide">ECCV2024 submission policies</a> (no more than
          14 pages) and use the official ECCV 2024 template. Note that the submissions violate the double-blind policy
          or the dual-submission policy will be rejected without review.</p>

        <h3>Call for extended abstracts and posters</h3>

        <p>Extended abstracts are not subject to the ECCV rules, and can use other templates. They should be shorter
          than the equivalent of 4 pages in CVPR template format. Note that extended abstracts are not anonymized, and
          will not be peer reviewed. Accepted extended abstracts will only publish on our website, show in our workshop
          and not appear in the ECCV24 conference proceedings. Posters can be simply submitted with PDF and their brief
          information like paper title, authors and conference name. </p>

        <p><b>The CMT system will close after August 15. After that, extended abstracts and posters can still be submitted via emails (hands2024@googlegroups.com) until August 20.</b></p> -->


        <h1 id="speakers">Invited Speakers</h1>
        <div id="member-container">
          <div id="member">
            <img src="./profiles/2024/han.jpg">
            <p>
              <b><a href="https://jhugestar.github.io">Hanbyul Joo</a></b></br>
              Seoul National University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/shunsuke.jpg">
            <p>
              <b><a href="https://shunsukesaito.github.io">Shunsuke Saito</a></b></br>
              Reality Labs Research
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/shubham.jpg">
            <p>
              <b><a href="https://shubhtuls.github.io">Shubham Tulsiani</a></b></br>
              Carnegie Mellon University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/yeqi.jpg">
            <p>
              <b><a href="https://person.zju.edu.cn/en/yeqi">Qi Ye</a></b></br>
              Zhejiang University
            </p>
          </div>
        </div>

        <h1 id="organizers">Organizers</h1>
        <p>
        <div id="member-container">
          <div id="member">
            <img src="./profiles/2024/hyung.jpg">
            <p>
              <b><a href="https://hyungjinchang.wordpress.com">Hyung Jin Chang</a></b></br>
              University of Birmingham
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/rongyu.jpg">
            <p>
              <b><a href="https://gloryyrolg.github.io">Rongyu Chen</a></b></br>
              National University of Singapore
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/zicong.png">
            <p>
              <b><a href="https://zc-alexfan.github.io">Zicong Fan</a></b></br>
              ETH Zurich
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/otmar.png">
            <p>
              <b><a href="https://ait.ethz.ch/people/hilliges">Otmar Hilliges</a></b></br>
              ETH Zurich
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/kun.png">
            <p>
              <b><a href="https://kunhe.github.io">Kun He</a></b></br>
              Meta Reality Labs
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/take.png">
            <p>
              <b><a href="https://tkhkaeio.github.io/">Take Ohkawa</a></b></br>
              University of Tokyo
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/yoichi.png">
            <p>
              <b><a href="https://sites.google.com/ut-vision.org/ysato/home">Yoichi Sato</a></b></br>
              University of Tokyo
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/elden.jpg">
            <p>
              <b><a href="https://eldentse.github.io">Elden Tse</a></b></br>
              National University of Singapore
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/linlin.png">
            <p>
              <b><a href="https://mu4yang.com">Linlin Yang</a></b></br>
              Communication University of China
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/lixin.png">
            <p>
              <b><a href="https://lixiny.github.io">Lixin Yang</a></b></br>
              Shanghai Jiao Tong University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/angela.png">
            <p>
              <b><a href="https://www.comp.nus.edu.sg/cs/people/ayao/">Angela Yao</a></b></br>
              National University of Singapore
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2024/linguang.png">
            <p>
              <b><a href="https://lg-zhang.github.io/">Linguang Zhang</a></b></br>
              Facebook Reality Labs (Oculus)
            </p>
          </div>
        </div>
        </p>

        <h1 id="sponsors">Technical Program Committee</h1>

        <p>Thank you so much to the Technical Program Committee for their thoughtful reviews.</p>

        <ul id="topicstyle1">
          <li id="topicstyle2">Chenyangguang Zhang (Tsinghua University)</li>
          <li id="topicstyle2">Gyeongsik Moon (Meta)</li>
          <li id="topicstyle2">Jiayin Zhu (NUS)</li>
          <li id="topicstyle2">Jihyun Lee (KAIST)</li>
          <li id="topicstyle2">Junuk Cha (UNIST)</li>
          <li id="topicstyle2">Kailin Li (Shanghai Jiao Tong University)</li>
          <li id="topicstyle2">Keyang Zhou (University of Tübingen)</li>
          <li id="topicstyle2">Pengzhan Sun (NUS)</li>
          <li id="topicstyle2">Rolandos Alexandros Potamias (Imperial College London)</li>
          <li id="topicstyle2">Seungryul Baek (UNIST)</li>
          <li id="topicstyle2">Takuma Yagi (AIST)</li>
          <li id="topicstyle2">Zerui Chen (Inria Paris)</li>
          <li id="topicstyle2">Zhiying Leng (Beihang University)</li>
          <li id="topicstyle2">Zhongqun Zhang (University of Birmingham)</li>
        </ul>


        <h1 id="sponsors">Sponsors</h1>
        <div class="sponsors-container">
          <img class="sponsor-img" src="./profiles/2024/sponsor1.png">
          <img class="sponsor-img" src="./profiles/2024/sponsor2_1.svg">
          <img class="sponsor-img" src="./profiles/2024/sponsor2_2.png">
        </div>



        <h1 id="contact">Contact</h1>
        <p>hands2024@googlegroups.com</p>


        <div id="footer">
          <p style="align-items: center;text-align: center;">

            <a href="https://youtube.com/@handsworkshop" target="_Blank">
              <img id="page1" alt="" src="profiles/youtube.jpg">
            </a>
            <a href="https://github.com/handsworkshop" target="_Blank">
              <img id="page" alt="" src="profiles/github.png">
            </a>
          </p>
        </div>
        <script>


          document.getElementById('outer_li_year').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            // 获取第一个<li>标签中的年份
            var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
            // 构建新的href
            var newHref = 'workshop' + year + '.html';
            // 跳转到新的页面

            window.location.href = newHref;



          });
          document.getElementById('workshop_link').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            // 获取第一个<li>标签中的年份
            var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
            // 构建新的href
            var newHref = 'workshop' + year + '.html';
            // 跳转到新的页面
            window.location.href = newHref;

          });
          document.getElementById('challenge_link').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            // 获取第一个<li>标签中的年份
            var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
            // 构建新的href
            var newHref = 'challenge' + year + '.html';
            // 跳转到新的页面
            window.location.href = newHref;

          });
          // 获取所有带有id="style3"的a标签
          var yearLinks = document.querySelectorAll('#style3');

          yearLinks.forEach(function (link) {
            link.addEventListener('click', function (event) {
              event.preventDefault(); // 阻止默认链接行为

              // 获取点击的年份
              var selectedYear = this.textContent.trim();

              // 更新第一个li中的年份显示
              document.getElementById('current_year').textContent = selectedYear;

              // 关闭下拉菜单
              // document.getElementById('top_list').style.display = 'none';

              // 可选：如果需要，可以在这里添加跳转逻辑
              // window.location.href = this.href;
            });
          });
          var workshopLi = document.querySelector('#workshop_link');
          workshopLi.classList.add('highlight');
          // function highlightLinks() {
          //   var currentHref = window.location.href;
          //   alert("执行")
          //   alert(currentHref)
          //   if (currentHref.includes('workshop')) {
          //     var workshopLi = document.querySelector('#workshop_link');
          //     workshopLi.classList.add('highlight');
          //   } else if (currentHref.includes('challenge')) {
          //     var workshopLi1 = document.querySelector('#challenge_link');
          //     workshopLi1.classList.add('highlight');
          //   }
          // }
          // window.addEventListener('popstate', highlightLinks);
        </script>
</body>

</html>
